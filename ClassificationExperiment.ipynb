{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14]]\n",
      "[[ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]] [-1. -1. -1. ..., -1. -1.  1.] [[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]] [-1. -1.  1. ..., -1. -1.  1.]\n",
      "loss train: 32561.0\n",
      " loss test: 16281.0\n",
      "total： 32561  error: 24720  acc rate: 0.2408095574460244\n",
      "total： 16281  error: 12435  acc rate: 0.2362262760272711\n",
      "delta_x: [-0.0, -0.0, -0.0, -0.0, -1.0, -1.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -1.0, -0.0, -0.0, -0.0, -0.0, -1.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -1.0, -0.0, -0.0, -1.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -1.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -1.0, -0.0, -0.0, -0.0, -0.0, -0.0, -1.0, -0.0, -0.0, -0.0, -0.0, -1.0, -0.0, -1.0, -0.0, -1.0, -0.0, -1.0, -0.0, -0.0, -0.0, -0.0, -1.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0]\n",
      "loss train: 57997.7\n",
      " loss test: 28414.7\n",
      "total： 32561  error: 7841  acc rate: 0.7591904425539756\n",
      "total： 16281  error: 3846  acc rate: 0.7637737239727289\n",
      "delta_x: [-0.0, -0.0, -0.0, -0.0, -0.14071950894605839, -0.14071950894605839, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.14071950894605839, -0.0, -0.0, -0.0, -0.0, -0.14071950894605839, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.14071950894605839, -0.0, -0.0, -0.14071950894605839, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.14071950894605839, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.14071950894605839, -0.0, -0.0, -0.0, -0.0, -0.0, -0.14071950894605839, -0.0, -0.0, -0.0, -0.0, -0.14071950894605839, -0.0, -0.14071950894605839, -0.0, -0.14071950894605839, -0.0, -0.14071950894605839, -0.0, -0.0, -0.0, -0.0, -0.14071950894605839, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0]\n",
      "loss train: 65055.8385594\n",
      " loss test: 31872.1077645\n",
      "total： 32561  error: 7841  acc rate: 0.7591904425539756\n",
      "total： 16281  error: 3846  acc rate: 0.7637737239727289\n",
      "delta_x: [-0.0, -0.0, -0.0, -0.0, -0.13948964628371432, -0.13948964628371432, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.13948964628371432, -0.0, -0.0, -0.0, -0.0, -0.13948964628371432, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.13948964628371432, -0.0, -0.0, -0.13948964628371432, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.13948964628371432, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.13948964628371432, -0.0, -0.0, -0.0, -0.0, -0.0, -0.13948964628371432, -0.0, -0.0, -0.0, -0.0, -0.13948964628371432, -0.0, -0.13948964628371432, -0.0, -0.13948964628371432, -0.0, -0.13948964628371432, -0.0, -0.0, -0.0, -0.0, -0.13948964628371432, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0]\n",
      "loss train: 72052.3176445\n",
      " loss test: 35299.3257805\n",
      "total： 32561  error: 7841  acc rate: 0.7591904425539756\n",
      "total： 16281  error: 3846  acc rate: 0.7637737239727289\n",
      "delta_x: [-0.0, -0.0, -0.0, -0.0, -0.1478156918753788, -0.1478156918753788, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.1478156918753788, -0.0, -0.0, -0.0, -0.0, -0.1478156918753788, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.1478156918753788, -0.0, -0.0, -0.1478156918753788, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.1478156918753788, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.1478156918753788, -0.0, -0.0, -0.0, -0.0, -0.0, -0.1478156918753788, -0.0, -0.0, -0.0, -0.0, -0.1478156918753788, -0.0, -0.1478156918753788, -0.0, -0.1478156918753788, -0.0, -0.1478156918753788, -0.0, -0.0, -0.0, -0.0, -0.1478156918753788, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0]\n",
      "loss train: 79466.4417099\n",
      " loss test: 38931.1419222\n",
      "total： 32561  error: 7841  acc rate: 0.7591904425539756\n",
      "total： 16281  error: 3846  acc rate: 0.7637737239727289\n",
      "delta_x: [-0.0, -0.0, -0.0, -0.0, 0.74117810048047517, -0.12347454638373247, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, 0.74117810048047517, -0.0, -0.0, -0.0, -0.0, -0.12347454638373247, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, 0.86465264686420762, -0.0, -0.0, -0.0, -0.0, -0.12347454638373247, -0.0, 0.86465264686420762, 0.74117810048047517, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.12347454638373247, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.12347454638373247, -0.0, 0.86465264686420762, -0.0, -0.0, -0.0, 0.74117810048047517, -0.0, -0.0, -0.0, -0.0, -0.12347454638373247, 0.86465264686420762, -0.12347454638373247, 0.86465264686420762, 0.74117810048047517, -0.0, 0.74117810048047517, -0.0, -0.0, -0.0, -0.0, 0.74117810048047517, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0]\n",
      "loss train: 41741.7988706\n",
      " loss test: 20480.1738678\n",
      "total： 32561  error: 7700  acc rate: 0.7635207763889316\n",
      "total： 16281  error: 3777  acc rate: 0.7680117928874148\n",
      "delta_x: [-0.0, -0.0, -0.0, -0.0, -0.071342304830194422, -0.16115318769156278, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.071342304830194422, -0.0, -0.0, -0.0, -0.0, -0.16115318769156278, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.089810882861368346, -0.0, -0.0, -0.0, -0.0, -0.16115318769156278, -0.0, -0.089810882861368346, -0.071342304830194422, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.16115318769156278, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.16115318769156278, -0.0, -0.089810882861368346, -0.0, -0.0, -0.0, -0.071342304830194422, -0.0, -0.0, -0.0, -0.0, -0.16115318769156278, -0.089810882861368346, -0.16115318769156278, -0.089810882861368346, -0.071342304830194422, -0.0, -0.071342304830194422, -0.0, -0.0, -0.0, -0.0, -0.071342304830194422, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0]\n",
      "loss train: 48490.9939284\n",
      " loss test: 23782.5648027\n",
      "total： 32561  error: 7767  acc rate: 0.7614631000276404\n",
      "total： 16281  error: 3813  acc rate: 0.765800626497144\n",
      "delta_x: [-0.0, -0.0, -0.0, -0.0, -0.077668096940369538, -0.17544234705717671, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.077668096940369538, -0.0, -0.0, -0.0, -0.0, -0.17544234705717671, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.079373983473568444, -0.0, -0.0, -0.0, -0.0, -0.17544234705717671, -0.0, -0.079373983473568444, -0.077668096940369538, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.17544234705717671, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.17544234705717671, -0.0, -0.079373983473568444, -0.0, -0.0, -0.0, -0.077668096940369538, -0.0, -0.0, -0.0, -0.0, -0.17544234705717671, -0.079373983473568444, -0.17544234705717671, -0.079373983473568444, -0.077668096940369538, -0.0, -0.077668096940369538, -0.0, -0.0, -0.0, -0.0, -0.077668096940369538, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss train: 55547.37769\n",
      " loss test: 27238.0270485\n",
      "total： 32561  error: 7828  acc rate: 0.759589693191241\n",
      "total： 16281  error: 3844  acc rate: 0.7638965665499662\n",
      "delta_x: [-0.0, -0.0, -0.0, -0.0, -0.084744263123708988, -0.19142650596253574, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.084744263123708988, -0.0, -0.0, -0.0, -0.0, -0.19142650596253574, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.070510738228879433, -0.0, -0.0, -0.0, -0.0, -0.19142650596253574, -0.0, -0.070510738228879433, -0.084744263123708988, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.19142650596253574, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.19142650596253574, -0.0, -0.070510738228879433, -0.0, -0.0, -0.0, -0.084744263123708988, -0.0, -0.0, -0.0, -0.0, -0.19142650596253574, -0.070510738228879433, -0.19142650596253574, -0.070510738228879433, -0.084744263123708988, -0.0, -0.084744263123708988, -0.0, -0.0, -0.0, -0.0, -0.084744263123708988, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0]\n",
      "loss train: 62974.1846237\n",
      " loss test: 30876.064834\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-a48056a8e9ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m \u001b[0mtrainAndTest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-a48056a8e9ea>\u001b[0m in \u001b[0;36mtrainAndTest\u001b[1;34m()\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" loss test:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0macc_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccRate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m         \u001b[0macc_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccRate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-a48056a8e9ea>\u001b[0m in \u001b[0;36maccRate\u001b[1;34m(m, X, y)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m             \u001b[0merror\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"total：\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\" error:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\" acc rate:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mm\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.externals.joblib import Memory\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "\n",
    "eta = 0.0001 # Learning Rate\n",
    "iter =50 # Iteration times\n",
    "accuracy = 0.001 # If loss<accuracy , then stop iteration\n",
    "lam = 0.1 #lambda used in Loss Function\n",
    "\n",
    "\n",
    "m_train = 32561 # Amount of training data\n",
    "m_test = 16281 # Amount of testing data\n",
    "features=123 # Fearures of dataset\n",
    "\n",
    "#Initialize arrays\n",
    "w= [0]*features  #All zero initialization\n",
    "\n",
    "#used to stastic and draw graph\n",
    "iter_num = [0]*iter;\n",
    "loss_train  = [0]*iter;\n",
    "loss_test  = [0]*iter;\n",
    "acc_train = [0]*iter;\n",
    "acc_test = [0]*iter;\n",
    "\n",
    "#used to calculate the step automatically\n",
    "#delta_x = [0]*features\n",
    "epsilon = 0.001\n",
    "history_x =  [0]*iter\n",
    "sum_of_x_squared = 0\n",
    "sum_of_grad_squared = 0\n",
    "current = 0\n",
    "\n",
    "#Use to get dataset from file\n",
    "def get_data():\n",
    "    data_train = load_svmlight_file(\"dataset\\\\a9a\",n_features=features)\n",
    "    X_train = data_train[0].toarray()\n",
    "    y_train = data_train[1]\n",
    "    data_test = load_svmlight_file(\"dataset\\\\a9a.t\",n_features=features)\n",
    "    X_test = data_test[0].toarray()\n",
    "    y_test = data_test[1]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "#Use to split dataset and return\n",
    "def split_data():\n",
    "    X, y = get_data()\n",
    "    X = X.toarray()\n",
    "    return train_test_split( X, y, test_size=0.5, random_state=43)\n",
    "\n",
    "#Loss function  \n",
    "def loss(m,X,y):\n",
    "    loss = 0.0\n",
    "    for i in range (0,m):\n",
    "        y_predict = 0\n",
    "        for j in range(0,features):\n",
    "            y_predict = y_predict + X[i][j] * w[j]\n",
    "        if(y[i]*y_predict-1<0):\n",
    "            loss += (1-y[i]*y_predict)\n",
    "    for  j in range (0,features):\n",
    "        loss = loss + 0.5 * lam * w[j] * w[j]\n",
    "    return loss\n",
    "\n",
    "\n",
    "#Derivative the Loss function to get Gradient(G)\n",
    "def derivative(m,X,y):\n",
    "    y_predict = [0]*m \n",
    "    for i in range (0,m):\n",
    "        y_predict[i] = 0\n",
    "        for j in range(0,features):\n",
    "            y_predict[i] = y_predict[i] + X[i][j] * w[j]\n",
    "    \n",
    "    grad = [0] * features\n",
    "    for j in range(0,features):\n",
    "        grad[j] = abs(lam*w[j])\n",
    "        for i in range(0,m):\n",
    "            if(y[i]*y_predict[i]-1<0):\n",
    "                grad[j] = grad[j] - y[i] * X[i][j]\n",
    "    return grad\n",
    "\n",
    "#Derivative the Loss function to get Gradient(G)\n",
    "def derivative_random(m,X,y):\n",
    "    i = random.randint(0,m-1)\n",
    "    y_predict = 0\n",
    "    for j in range(0,features):\n",
    "        y_predict = y_predict + X[i][j] * w[j]\n",
    "    \n",
    "    grad = [0] * features\n",
    "    for j in range(0,features):\n",
    "        grad[j] = abs(lam*w[j])\n",
    "        if(y[i]*y_predict-1<0):\n",
    "             grad[j] = grad[j] - y[i] * X[i][j]\n",
    "    \n",
    "    return grad\n",
    "\n",
    "#Process Gradient descent to minimum the Loss\n",
    "def update_random(m,X,y):\n",
    "    # grad is the gradient G\n",
    "    grad = derivative_random(m,X,y)\n",
    "    delta_of_grad = delta_x(grad)\n",
    "    for j in range(0,features):\n",
    "         #\" D = -G \" is \" - grad[j] \" here\n",
    "        w[j] = w[j] + delta_of_grad[j]\n",
    "\n",
    "def  squared(vector):\n",
    "    a = np.mat(vector)\n",
    "    b = a*a.T\n",
    "    return b[0][0]\n",
    "\n",
    "def multiply(vector1,vector2):\n",
    "    c= [0]*len(vector1)\n",
    "    for i in range(0,len(vector1)):\n",
    "        c[i] = vector1[i] * vector2[i]\n",
    "    return c\n",
    "        \n",
    "def delta_x(grad):\n",
    "    return adadelta(grad)\n",
    "\n",
    "def adadelta(grad):\n",
    "    global sum_of_grad_squared, sum_of_x_squared, current\n",
    "    \n",
    "    delta_x = [0]*features\n",
    "    sum_of_grad_squared += squared(grad)\n",
    "    \n",
    "    if(current==0):\n",
    "        current+=1\n",
    "        delta_x =[ x * -1.0 for x in grad ]\n",
    "    else:\n",
    "        RMS_delta_x = math.sqrt(sum_of_x_squared/current)\n",
    "        RMS_grad = math.sqrt(sum_of_grad_squared/(current+1))\n",
    "        eta = -1.0 * ( RMS_delta_x / RMS_grad)\n",
    "        delta_x =[ x * eta for x in grad ]\n",
    "        current+=1\n",
    "    sum_of_x_squared += squared(delta_x)\n",
    "    print(\"delta_x:\",delta_x)\n",
    "    return delta_x\n",
    "    \n",
    "    \n",
    "#Train and validate \n",
    "def trainAndTest():\n",
    "    for i in range (0,iter):\n",
    "        iter_num[i] = i;\n",
    "        \n",
    "                \n",
    "        loss_train[i] = loss(m_train,X_train,y_train)\n",
    "        print(\"loss train:\",loss_train[i]);\n",
    "        \n",
    "        loss_test[i] = loss(m_test,X_test,y_test)\n",
    "        print(\" loss test:\",loss_test[i]);\n",
    "        \n",
    "        acc_train[i] = accRate(m_train,X_train,y_train)\n",
    "        acc_test[i] = accRate(m_test,X_test,y_test)\n",
    "            \n",
    "        for j in range (0,1):\n",
    "            update_random(m_train,X_train,y_train)\n",
    "        \n",
    "        \n",
    "#The linear model\n",
    "def predict(x):\n",
    "    pre = 0.0\n",
    "    for j in range (0,features):\n",
    "        pre = pre + x[j] * w[j]\n",
    "    if(pre>=0) :\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def accRate(m,X,y):\n",
    "    error = 0\n",
    "    for i in range(0,m):\n",
    "        if(predict(X[i])!=y[i]):\n",
    "            error+=1\n",
    "    print(\"total：\",m,\" error:\",error,\" acc rate:\",1.0 - error/m )\n",
    "    return 1.0 - error/m\n",
    "    \n",
    "\n",
    "#Test\n",
    "\n",
    "a = [1,2,3]\n",
    "b = [2,2,3]\n",
    "c = squared(a)\n",
    "\n",
    "print(c)\n",
    "    \n",
    "    \n",
    "#Main \n",
    "X_train, y_train, X_test,y_test = get_data()\n",
    "print(X_train, y_train, X_test,y_test)\n",
    "trainAndTest()\n",
    "\n",
    "\n",
    "#Print the information and draw graphs\n",
    "print(\"Features:\",w)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(iter_num, loss_train,color = 'm', label='loss of train')\n",
    "ax.plot(iter_num, loss_test, color = 'c', label='loss of validation')\n",
    "plt.legend(bbox_to_anchor=[1, 1])  \n",
    "ax.set_xlabel('Iteration times')\n",
    "ax.set_ylabel('Loss')\n",
    "plt.show()  \n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.plot(iter_num, acc_train,color = 'm', label='Acc of train')\n",
    "ax2.plot(iter_num, acc_test, color = 'c', label='Acc of validation')\n",
    "plt.legend(bbox_to_anchor=[1, 1])  \n",
    "ax2.set_xlabel('Iteration times')\n",
    "ax2.set_ylabel('Accuracy Rate')\n",
    "plt.show()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
