{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25917 22594 14528]\n",
      "[[ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]] [-1. -1. -1. ..., -1. -1.  1.] [[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]] [-1. -1.  1. ..., -1. -1.  1.]\n",
      " loss test nag: 3257.0\n",
      " loss test nag: 1678.89287864\n",
      " loss test nag: 1357.2708204\n",
      " loss test nag: 1176.46786037\n",
      " loss test nag: 1184.99199476\n",
      " loss test nag: 1145.88998936\n",
      " loss test nag: 1123.61616008\n",
      " loss test nag: 1200.39122657\n",
      " loss test nag: 1190.13672078\n",
      " loss test nag: 1156.80228721\n",
      " loss test nag: 1183.27759459\n",
      " loss test nag: 1148.06069438\n",
      " loss test nag: 1220.03802867\n",
      " loss test nag: 1092.61750947\n",
      " loss test nag: 1168.56166835\n",
      " loss test nag: 1147.20124174\n",
      " loss test nag: 1189.46422711\n",
      " loss test nag: 1154.27759141\n",
      " loss test nag: 1165.1194458\n",
      " loss test nag: 1159.4349263\n",
      " loss test nag: 1112.890298\n",
      " loss test nag: 1102.15789623\n",
      " loss test nag: 1166.6597779\n",
      " loss test nag: 1156.35591936\n",
      " loss test nag: 1125.6599415\n",
      " loss test nag: 1171.72893076\n",
      " loss test nag: 1213.01695809\n",
      " loss test nag: 1174.96999753\n",
      " loss test nag: 1198.17913697\n",
      " loss test nag: 1196.31687118\n",
      " loss test nag: 1211.32985914\n",
      " loss test nag: 1214.8904523\n",
      " loss test nag: 1195.99591214\n",
      " loss test nag: 1179.74825661\n",
      " loss test nag: 1160.95970187\n",
      " loss test nag: 1163.03238457\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals.joblib import Memory\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "\n",
    "eta = 0.0001 # Learning Rate\n",
    "epsilon = 0.00001 #using in 4 optimalization methods to prevent the denominator become 0\n",
    "iter =40# Iteration times\n",
    "accuracy = 0.001 # If loss<accuracy , then stop iteration\n",
    "lam = 0.1 #lambda used in Loss Function\n",
    "mini_batch_percent = 0.2 #5% of dataset is used in loss function\n",
    "\n",
    "m_train = 32561 # Amount of training data\n",
    "m_test = 16281 # Amount of testing data\n",
    "features=123 # Fearures of dataset\n",
    "\n",
    "#Initialize arrays\n",
    "w= [0]*features  #All zero initialization\n",
    "\n",
    "#used to stastic and draw graph\n",
    "iter_num = [0]*iter;\n",
    "loss_train  = [0]*iter;\n",
    "loss_test_nag  = [0]*iter;\n",
    "loss_test_rmsprop  = [0]*iter;\n",
    "loss_test_adadelta  = [0]*iter;\n",
    "loss_test_adam  = [0]*iter;\n",
    "\n",
    "\n",
    "#used in adadelta\n",
    "sum_of_x_squared = 0\n",
    "sum_of_grad_squared = 0\n",
    "current = 0\n",
    "\n",
    "#used in adam\n",
    "mt = np.zeros(features)\n",
    "vt =0 \n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "\n",
    "#used in rmsProp\n",
    "Eg2 = 0\n",
    "\n",
    "#used in NAG\n",
    "v = np.zeros(features)\n",
    "miu = 0.9\n",
    "\n",
    "#Use to get dataset from file\n",
    "def get_data():\n",
    "    data_train = load_svmlight_file(\"dataset\\\\a9a\",n_features=features)\n",
    "    X_train = data_train[0].toarray()\n",
    "    y_train = data_train[1]\n",
    "    data_test = load_svmlight_file(\"dataset\\\\a9a.t\",n_features=features)\n",
    "    X_test = data_test[0].toarray()\n",
    "    y_test = data_test[1]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def  squared(vector):\n",
    "    a = np.mat(vector)\n",
    "    b = a*a.T\n",
    "    return b[0][0]\n",
    "\n",
    "def multiply(vector1,vector2):\n",
    "    c= [0]*len(vector1)\n",
    "    for i in range(0,len(vector1)):\n",
    "        c[i] = vector1[i] * vector2[i]\n",
    "    return c\n",
    "\n",
    "#Loss function  \n",
    "def loss(m,X,y):\n",
    "    loss = 0.0\n",
    "    for i in range (0,m):\n",
    "        y_predict = 0\n",
    "        for j in range(0,features):\n",
    "            y_predict = y_predict + X[i][j] * w[j]\n",
    "        if(y[i]*y_predict-1<0):\n",
    "            loss += (1-y[i]*y_predict)\n",
    "    for  j in range (0,features):\n",
    "        loss = loss + 0.5 * lam * w[j] * w[j]\n",
    "    return loss\n",
    "\n",
    "#mini -batch loss function  \n",
    "def SGD_loss(m,X,y):\n",
    "    loss = 0.0\n",
    "    sample_num =math.ceil( m * mini_batch_percent)\n",
    "    target = np.random.permutation(m)[0:sample_num]\n",
    "    for index in range (0,sample_num):\n",
    "        i = target[index]\n",
    "        y_predict = 0\n",
    "        for j in range(0,features):\n",
    "            y_predict = y_predict + X[i][j] * w[j]\n",
    "        if(y[i]*y_predict-1<0):\n",
    "            loss += (1-y[i]*y_predict)\n",
    "    for  j in range (0,features):\n",
    "        loss = loss + 0.5 * lam * w[j] * w[j]\n",
    "    return loss\n",
    "\n",
    "\n",
    "#Derivative the Loss function to get Gradient(G)\n",
    "def derivative(m,X,y):\n",
    "    y_predict = [0]*m \n",
    "    for i in range (0,m):\n",
    "        y_predict[i] = 0\n",
    "        for j in range(0,features):\n",
    "            y_predict[i] = y_predict[i] + X[i][j] * w[j]\n",
    "    \n",
    "    grad = [0] * features\n",
    "    for j in range(0,features):\n",
    "        grad[j] = abs(lam*w[j])\n",
    "        for i in range(0,m):\n",
    "            if(y[i]*y_predict[i]-1<0):\n",
    "                grad[j] = grad[j] - y[i] * X[i][j]\n",
    "    return grad\n",
    "\n",
    "#Derivative the Loss function to get Gradient(G)\n",
    "def SGD_grad(m,X,y):\n",
    "    \n",
    "    sample_num =math.ceil( m * mini_batch_percent)\n",
    "    target=random.sample(range(0,m),sample_num)\n",
    "    \n",
    "    y_predict = [0]*m\n",
    "    for index in range (0,sample_num):\n",
    "        i = target[index]\n",
    "        y_predict[i] = 0\n",
    "        for j in range(0,features):\n",
    "            y_predict[i] = y_predict[i] + X[i][j] * w[j]\n",
    "    \n",
    "    grad = [0] * features\n",
    "    for j in range(0,features):\n",
    "        grad[j] = abs(lam*w[j])\n",
    "        for index in range(0,sample_num):\n",
    "            i = target[index]\n",
    "            if(y[i]*y_predict[i]-1<0):\n",
    "                grad[j] = grad[j] - y[i] * X[i][j]\n",
    "    return grad\n",
    "\n",
    "#Derivative the Loss function to get Gradient(G)\n",
    "def derivative_random(m,X,y):\n",
    "    i = random.randint(0,m-1)\n",
    "    y_predict = 0\n",
    "    for j in range(0,features):\n",
    "        y_predict = y_predict + X[i][j] * w[j]\n",
    "    \n",
    "    grad = [0] * features\n",
    "    for j in range(0,features):\n",
    "        grad[j] = abs(lam*w[j])\n",
    "        if(y[i]*y_predict-1<0):\n",
    "             grad[j] = grad[j] - y[i] * X[i][j]\n",
    "    \n",
    "    return grad\n",
    "\n",
    "def update(m,X,y,method):\n",
    "    global w\n",
    "    if (method=='nag'):\n",
    "        grad = SGD_grad(m,X,y)\n",
    "        w = w + NAG(grad)\n",
    "    if (method=='rmsprop'):\n",
    "        grad = SGD_grad(m,X,y)\n",
    "        w = w + rmsProp(grad)\n",
    "    if (method=='adadelta'):\n",
    "        grad = SGD_grad(m,X,y)\n",
    "        w = w + adadelta(grad)\n",
    "    if (method=='adam'):\n",
    "        grad = SGD_grad(m,X,y)\n",
    "        w = w + adam(grad)\n",
    "        \n",
    "def adadelta(grad):\n",
    "    global sum_of_grad_squared, sum_of_x_squared, current\n",
    "    sum_of_grad_squared += squared(grad)\n",
    "    grad2 = np.mat(grad)\n",
    "    delta_x_mat =  -0.0001* grad2\n",
    "    \n",
    "    if(current==0):\n",
    "        current+=1\n",
    "        delta_x_mat = -0.0001*  grad2\n",
    "    else:\n",
    "        RMS_delta_x = math.sqrt(sum_of_x_squared/current)\n",
    "        RMS_grad = math.sqrt(sum_of_grad_squared/(current+1))\n",
    "        alpha = -1.0 * ( RMS_delta_x / RMS_grad)\n",
    "        delta_x_mat = alpha * grad2\n",
    "        current+=1\n",
    "    sum_of_x_squared += squared(delta_x_mat.getA1())\n",
    "    \n",
    "    \n",
    "    return delta_x_mat.getA1()\n",
    "\n",
    "def adam(grad):\n",
    "    global beta1,beta2,mt,vt,epsilon,current\n",
    "    current+=1\n",
    "    delta_x = [0]*features\n",
    "    grad2 = np.mat(grad)\n",
    "    mt = beta1 * mt + (1-beta1)*(grad2)\n",
    "    vt = beta2 * vt + (1-beta2)*( (grad2 * grad2.T)[0,0])\n",
    "    delta_x_mat = -eta *100 * (math.sqrt(1-beta2**current)/(1-beta1**current))*(mt/(math.sqrt(vt)+epsilon)) \n",
    "    \n",
    "    return delta_x_mat.getA1()\n",
    "    \n",
    "def rmsProp(grad):\n",
    "    global Eg2\n",
    "    grad2 = np.mat(grad)\n",
    "    gt2 = (grad2 * grad2.T)[0,0]\n",
    "    Eg2 =  0.9 * Eg2 + 0.1 * gt2\n",
    "    delta_x_mat =  -eta * 100 / (math.sqrt(Eg2+epsilon)) *(grad2)\n",
    "    return delta_x_mat.getA1()\n",
    "    \n",
    "def NAG(grad):\n",
    "    global v,eta,miu\n",
    "    grad2 = np.mat(grad)\n",
    "    v = miu * v  - eta * grad2\n",
    "    return v.getA1()\n",
    "    \n",
    "\n",
    "'''\n",
    "#Process Gradient descent to minimum the Loss\n",
    "def update(m,X,y):\n",
    "    # grad is the gradient G\n",
    "    grad = derivative_random(m,X,y)\n",
    "    delta_of_grad = delta_x(grad)\n",
    "    for j in range(0,features):\n",
    "         #\" D = -G \" is \" - grad[j] \" here\n",
    "        w[j] = w[j] + delta_of_grad[j]\n",
    "\n",
    "\n",
    "        \n",
    "def delta_x(grad):\n",
    "    return NAG(grad)\n",
    "\n",
    "def adadelta(grad):\n",
    "    global sum_of_grad_squared, sum_of_x_squared, current\n",
    "    \n",
    "    delta_x = [0]*features\n",
    "    sum_of_grad_squared += squared(grad)\n",
    "    \n",
    "    if(current==0):\n",
    "        current+=1\n",
    "        delta_x =[ x * -0.0001 for x in grad ]\n",
    "    else:\n",
    "        RMS_delta_x = math.sqrt(sum_of_x_squared/current)\n",
    "        RMS_grad = math.sqrt(sum_of_grad_squared/(current+1))\n",
    "        eta = -1.0 * ( RMS_delta_x / RMS_grad)\n",
    "        delta_x =[ x * eta for x in grad ]\n",
    "        current+=1\n",
    "    sum_of_x_squared += squared(delta_x)\n",
    "   # print(\"delta_x:\",delta_x)\n",
    "    return delta_x\n",
    "\n",
    "def adam(grad):\n",
    "    global beta1,beta2,mt,vt,epsilon,current\n",
    "    current+=1\n",
    "    delta_x = [0]*features\n",
    "    grad2 = np.mat(grad)\n",
    "    mt = beta1 * mt + (1-beta1)*(grad2)\n",
    "    vt = beta2 * vt + (1-beta2)*( (grad2 * grad2.T)[0,0])\n",
    "    delta_x_mat = -eta * (math.sqrt(1-beta2**current)/(1-beta1**current))*(mt/(math.sqrt(vt)+epsilon)) \n",
    "    \n",
    "    return delta_x_mat.getA1()\n",
    "    \n",
    "def rmsProp(grad):\n",
    "    global Eg2\n",
    "    grad2 = np.mat(grad)\n",
    "    gt2 = (grad2 * grad2.T)[0,0]\n",
    "    Eg2 =  0.9 * Eg2 + 0.1 * gt2\n",
    "    delta_x_mat =  -eta / (math.sqrt(Eg2+epsilon)) *(grad2)\n",
    "    return delta_x_mat.getA1()\n",
    "    \n",
    "def NAG(grad):\n",
    "    global v,eta,miu\n",
    "    grad2 = np.mat(grad)\n",
    "    v = miu * v  - eta * grad2\n",
    "    return v.getA1()\n",
    " '''   \n",
    "    \n",
    "#Train and validate \n",
    "def trainAndTest():\n",
    "    \n",
    "    global w\n",
    "    \n",
    "    \n",
    "    for i in range (0,iter):\n",
    "        iter_num[i] = i;\n",
    "                   \n",
    "        loss_test_nag[i] =  SGD_loss(m_test,X_test,y_test)\n",
    "        print(\" loss test nag:\",loss_test_nag[i]);\n",
    "        \n",
    "        for j in range (0,10):\n",
    "            update(m_train,X_train,y_train,'nag')\n",
    "    \n",
    "    w = [0]*features\n",
    "  \n",
    "    for i in range (0,iter):\n",
    "        iter_num[i] = i;\n",
    "                   \n",
    "        loss_test_rmsprop[i] =  SGD_loss(m_test,X_test,y_test)\n",
    "        print(\" loss test rmsProp:\",loss_test_rmsprop[i]);\n",
    "        \n",
    "        for j in range (0,10):\n",
    "            update(m_train,X_train,y_train,'rmsprop')\n",
    "    \n",
    "    w = [0]*features    \n",
    "   \n",
    "    for i in range (0,iter):\n",
    "        iter_num[i] = i;\n",
    "\n",
    "        loss_test_adadelta[i] =  SGD_loss(m_test,X_test,y_test)\n",
    "        print(\" loss test adadelta:\", loss_test_adadelta[i]);\n",
    "        \n",
    "        for j in range (0,10):\n",
    "            update(m_train,X_train,y_train,'adadelta')\n",
    "    \n",
    "    w = [0]*features\n",
    "           \n",
    "    for i in range (0,iter):\n",
    "        iter_num[i] = i;\n",
    "                   \n",
    "        loss_test_adam[i] =  SGD_loss(m_test,X_test,y_test)\n",
    "        print(\" loss test adam:\",loss_test_adam[i]);\n",
    "        \n",
    "        for j in range (0,10):\n",
    "            update(m_train,X_train,y_train,'adam')\n",
    "        \n",
    "#The linear model\n",
    "def predict(x):\n",
    "    pre = 0.0\n",
    "    for j in range (0,features):\n",
    "        pre = pre + x[j] * w[j]\n",
    "    if(pre>=0) :\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def accRate(m,X,y):\n",
    "    error = 0\n",
    "    for i in range(0,m):\n",
    "        if(predict(X[i])!=y[i]):\n",
    "            error+=1\n",
    "    print(\"totalï¼š\",m,\" error:\",error,\" acc rate:\",1.0 - error/m )\n",
    "    return 1.0 - error/m\n",
    "    \n",
    "\n",
    "#Test\n",
    "print(np.random.permutation(m_train)[0:3])\n",
    "    \n",
    "#Main \n",
    "X_train, y_train, X_test,y_test = get_data()\n",
    "print(X_train, y_train, X_test,y_test)\n",
    "trainAndTest()\n",
    "\n",
    "\n",
    "#Print the information and draw graphs\n",
    "print(\"Features:\",w)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(iter_num, loss_test_nag,  label='loss of NAG')\n",
    "ax.plot(iter_num, loss_test_rmsprop,  label='loss of RMSProp')\n",
    "ax.plot(iter_num, loss_test_adadelta,  label='loss of AdaDelta')\n",
    "ax.plot(iter_num, loss_test_adam,  label='loss of Adam')\n",
    "plt.legend(bbox_to_anchor=[1, 1])  \n",
    "ax.set_xlabel('Iteration times')\n",
    "ax.set_ylabel('Loss')\n",
    "plt.show()  \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
