{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals.joblib import Memory\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "eta = 0.0001 # Learning Rate\n",
    "iter =200 # Iteration times\n",
    "accuracy = 0.001 # If loss<accuracy , then stop iteration\n",
    "lam = 0.0001 #lambda used in Loss Function\n",
    "\n",
    "m_train = 32561 # Amount of training data\n",
    "m_test = 16281 # Amount of testing data\n",
    "features=123 # Fearures of dataset\n",
    "\n",
    "#Initialize arrays\n",
    "w= [0]*features  #All zero initialization\n",
    "\n",
    "#used to stastic and draw graph\n",
    "iter_num = [0]*iter;\n",
    "loss_train  = [0]*iter;\n",
    "loss_test  = [0]*iter;\n",
    "acc_train = [0]*iter;\n",
    "acc_test = [0]*iter;\n",
    "\n",
    "#Use to get dataset from file\n",
    "def get_data():\n",
    "    data_train = load_svmlight_file(\"dataset\\\\a9a\",n_features=features)\n",
    "    X_train = data_train[0].toarray()\n",
    "    y_train = data_train[1]\n",
    "    data_test = load_svmlight_file(\"dataset\\\\a9a.t\",n_features=features)\n",
    "    X_test = data_test[0].toarray()\n",
    "    y_test = data_test[1]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "#Use to split dataset and return\n",
    "def split_data():\n",
    "    X, y = get_data()\n",
    "    X = X.toarray()\n",
    "    return train_test_split( X, y, test_size=0.5, random_state=43)\n",
    "\n",
    "#Loss function  \n",
    "def loss(m,X,y):\n",
    "    loss = 0.0\n",
    "    for i in range (0,m):\n",
    "        y_predict = 0\n",
    "        for j in range(0,features):\n",
    "            y_predict = y_predict + X[i][j] * w[j]\n",
    "        if(y[i]*y_predict-1<0):\n",
    "            loss += (1-y[i]*y_predict)\n",
    "    for  j in range (0,features):\n",
    "        loss = loss + 0.5 * lam * w[j] * w[j]\n",
    "    return loss\n",
    "\n",
    "\n",
    "#Derivative the Loss function to get Gradient(G)\n",
    "def derivative(m,X,y):\n",
    "    y_predict = [0]*m \n",
    "    for i in range (0,m):\n",
    "        y_predict[i] = 0\n",
    "        for j in range(0,features):\n",
    "            y_predict[i] = y_predict[i] + X[i][j] * w[j]\n",
    "    \n",
    "    grad = [0] * features\n",
    "    for j in range(0,features):\n",
    "        grad[j] = abs(lam*w[j])\n",
    "        for i in range(0,m):\n",
    "            if(y[i]*y_predict[i]-1<0):\n",
    "                grad[j] = grad[j] - y[i] * X[i][j]\n",
    "    return grad\n",
    "                \n",
    "#Process Gradient descent to minimum the Loss\n",
    "def update(m,X,y):\n",
    "    # grad is the gradient G\n",
    "    grad = derivative(m,X,y)\n",
    "    for j in range(0,features):\n",
    "         #\" D = -G \" is \" - grad[j] \" here\n",
    "        w[j] = w[j] - eta*grad[j]\n",
    "        \n",
    "#Train and validate \n",
    "def trainAndTest():\n",
    "    for i in range (0,iter):\n",
    "        iter_num[i] = i;\n",
    "        \n",
    "                \n",
    "        loss_train[i] = loss(m_train,X_train,y_train)\n",
    "        print(\"loss train:\",loss_train[i]);\n",
    "        \n",
    "        loss_test[i] = loss(m_test,X_test,y_test)\n",
    "        print(\" loss test:\",loss_test[i]);\n",
    "        \n",
    "        acc_train[i] = accRate(m_train,X_train,y_train)\n",
    "        acc_test[i] = accRate(m_test,X_test,y_test)\n",
    "            \n",
    "        update(m_train,X_train,y_train)\n",
    "        \n",
    "#The linear model\n",
    "def predict(x):\n",
    "    pre = 0.0\n",
    "    for j in range (0,features):\n",
    "        pre = pre + x[j] * w[j]\n",
    "    if(pre>=0) :\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def accRate(m,X,y):\n",
    "    error = 0\n",
    "    for i in range(0,m):\n",
    "        if(predict(X[i])!=y[i]):\n",
    "            error+=1\n",
    "    print(\"totalï¼š\",m,\" error:\",error,\" acc rate:\",1.0 - error/m )\n",
    "    return 1.0 - error/m\n",
    "    \n",
    "    \n",
    "#Main \n",
    "X_train, y_train, X_test,y_test = split_data()\n",
    "\n",
    "    \n",
    "trainAndTest()\n",
    "\n",
    "\n",
    "#Print the information and draw graphs\n",
    "print(\"Features:\",w)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(iter_num, loss_train,color = 'm', label='loss of train')\n",
    "ax.plot(iter_num, loss_test, color = 'c', label='loss of validation')\n",
    "plt.legend(bbox_to_anchor=[1, 1])  \n",
    "ax.set_xlabel('Iteration times')\n",
    "ax.set_ylabel('Loss')\n",
    "plt.show()  \n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.plot(iter_num, acc_train,color = 'm', label='Acc of train')\n",
    "ax2.plot(iter_num, acc_test, color = 'c', label='Acc of validation')\n",
    "plt.legend(bbox_to_anchor=[1, 1])  \n",
    "ax2.set_xlabel('Iteration times')\n",
    "ax2.set_ylabel('Accuracy Rate')\n",
    "plt.show()\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
